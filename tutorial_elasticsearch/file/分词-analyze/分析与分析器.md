### analysis 分析
1. 将一块文本分成适合于倒排索引的独立的词条  
2. 将这些词条统一化为标准格式以提高它们的"可搜索性"

### analyzer 分析器
**分析器执行上面的工作.分析器实际上是将三个功能封装到了一个包里,按顺序执行**  
1. CharFilters(Character filters) 字符过滤器
```
字符过滤器用来整理一个尚未被分词的字符串,例如去除HTML标签,把&Aacute;转换为相对应的Unicode字符Á. 
```
*一个分析器可能有0个或者多个字符过滤器*  

2. tokenizer 分词器
```
一个分析器必须有一个唯一的分词器.分词器把字符串分解成单个词条或者词汇单元.
关键词分词器完整地输出接收到的字符串,并不做任何分词.
空格分词器只根据空格分割文本.
正则分词器根据匹配正则表达式来分割文本. 
```

3. token filtering 词单元过滤器
```
经过分词作为结果的词单元流会按照指定的顺序通过指定的词单元过滤器.
词单元过滤器可以修改、添加或者移除词单元.
lowercase小写词过滤器处理.
stop停止词过滤器移除自定义的停止词列表中包含的词.
词干过滤器把单词转换为词干.
ascii_folding过滤器移除变音符,把一个像"très"这样的词转换为"tres".
ngram和edge_ngram词单元过滤器可以产生适合用于部分匹配或者自动补全的词单元.
```

*去除html----->按照规则分词----->修改过滤分词(转大小写、去除停止词、自动补全)*




#### 示例
```
curl -XPUT 'localhost:9200/my_index?pretty' -H 'Content-Type: application/json' -d'
{
    "settings": {
        "analysis": {
            "char_filter": {
                "&_to_and": {
                    "type":       "mapping",
                    "mappings": [ "&=> and "]
            }},
             "filter": {
                "my_stopwords": {
                    "type":       "stop",
                    "stopwords": [ "the", "a" ]
            }},
             "analyzer": {
                "my_analyzer": {
                    "type":         "custom",
                    "char_filter":  [ "html_strip", "&_to_and" ],
                    "tokenizer":    "standard",
                    "filter":       [ "lowercase", "my_stopwords" ]
            }}
}}}
'
```

#### 模板
```
PUT /my_index
{
    "settings": {
        "analysis": {
            "char_filter": { ... custom character filters ... },
            "tokenizer":   { ...    custom tokenizers     ... },
            "filter":      { ...   custom token filters   ... },
            "analyzer":    { ...    custom analyzers      ... }
        }
    }
}
```